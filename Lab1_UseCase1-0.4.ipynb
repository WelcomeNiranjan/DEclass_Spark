{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usecase-1**  \n",
    "\n",
    "Reading: Load employees table from MySQL database into Spark. \n",
    "\n",
    "Requirement: The report must include following information:   \n",
    "1. Calculate the employees current age.\n",
    "2. Calculate total number of years worked by employee.\n",
    "3. Find the age of employee when they are hired at the company.\n",
    "4. Show employee birth year.\n",
    "5. Create employee abbreviated name that contains 2 first character from last name and all character from first in lower case.\n",
    "6. Reverse employee number.\n",
    "\n",
    "Ordering: Sort the data by employee abbreviated name in ascending order.\n",
    "\n",
    "The ordinality, attributes name and type is defined below:\n",
    "6. id (type: integer)\n",
    "5. user (type: string)\n",
    "1. age (type: integer)\n",
    "4. birth_year (type: integer)\n",
    "3. start_age (type: integer)\n",
    "2. year_worked (type: integer)\n",
    "\n",
    "Output 1: The output file must be written in each file type shown below. The directory structure is defined below:   \n",
    "BASE_DIR = `/opt/spark_processing/data/employee`   \n",
    "FILE_TYPE = { `csv | parquet | orc | avro | json` }   \n",
    "DATA = { `employee` }   \n",
    "CURRENT_DATE = `now()`   \n",
    "FILE_NAME = `spark_$DATA_$CURRENT_DATE`   \n",
    "LOCATION = `$BASE_DIR/$FILE_NAME/$DATA/$FILE_NAME`     \n",
    " \n",
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reading: Load employees table from MySQL database into Spark.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dipti\\\\spark-3.0.0-preview2-bin-hadoop3.2\\\\spark-3.0.0-preview2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('UseCase_1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config_filename = '.\\db_properties.ini'\n",
    "db_properties = {}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_filename)\n",
    "db_prop = config['mysql']\n",
    "db_url = db_prop ['url']#remove employees from 'url = jdbc:mysql://localhost:3306/employees' in db_properties.ini file\n",
    "db_properties ['driver'] = db_prop['driver']\n",
    "db_properties['user'] = db_prop['user']\n",
    "db_properties['password'] = db_prop['password']\n",
    "db_properties['timezone'] = db_prop ['timezone']\n",
    "#db_properties['database'] = db_prop['database'] # remove database = employees from db_properties.inifile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read.jdbc(url = db_url, table = 'employees',\\\n",
    "                               properties = db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|\n",
      "+------+----------+----------+---------+------+----------+---+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|\n",
      "+------+----------+----------+---------+------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the employees current age.\n",
    "from pyspark.sql.functions import floor\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import current_date, datediff\n",
    "curr_age = (datediff(current_date(), col(\"birth_date\")))/365\n",
    "age = employees_df.select('*', floor(curr_age).alias(\"age\").cast(\"Integer\"))\n",
    "age.show(5)\n",
    "age.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate total number of years worked by employee.\n",
    "from pyspark.sql.functions import round\n",
    "years = datediff(current_date(), \"hire_date\")/365\n",
    "year_worked = age.select('*', round(years, 0).cast('Integer').alias(\"year_worked\"))\n",
    "year_worked.show(5)\n",
    "year_worked.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the age of employee when they are hired at the company.\n",
    "age_when_hired = datediff(\"hire_date\", \"birth_date\")/365\n",
    "start_age = year_worked.select('*', floor(age_when_hired).cast('Integer').alias(\"start_age\"))\n",
    "start_age.show(5)\n",
    "start_age.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|birth_year|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|      1953|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|      1964|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|      1959|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|      1954|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|      1955|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show employee birth year.\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "birth_year = start_age.select('*',(regexp_replace(\"birth_date\", r'-\\d{2}-\\d{2}$', '')\\\n",
    "                                  .cast('Integer').alias(\"birth_year\")))\n",
    "birth_year.show(5)\n",
    "birth_year.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|birth_year|       user|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|      1953|   fageorgi|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|      1964|  sibezalel|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|      1959|    baparto|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|      1954|kochirstian|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|      1955|  makyoichi|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create employee abbreviated name that contains 2 first character from last name \n",
    "#and all character from first in lower case.\n",
    "from pyspark.sql.functions import concat, substring, lower, col\n",
    "\n",
    "l_name_sub = substring(\"last_name\", 0, 2)\n",
    "conct = lower(concat(l_name_sub, col(\"first_name\")))\n",
    "user = birth_year.select('*', conct.alias(\"user\"))\n",
    "user.show(5)\n",
    "user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+-----+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|birth_year|       user|   id|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+-----+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|      1953|   fageorgi|10001|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|      1964|  sibezalel|20001|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|      1959|    baparto|30001|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|      1954|kochirstian|40001|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|      1955|  makyoichi|50001|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reverse employee number\n",
    "from pyspark.sql.functions import reverse\n",
    "\n",
    "id = user.select('*', reverse(\"emp_no\").cast('Integer').alias(\"id\"))\n",
    "id.show(5)\n",
    "id.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+----------+---------+-----------+\n",
      "|    id|         user|age|birth_year|start_age|year_worked|\n",
      "+------+-------------+---+----------+---------+-----------+\n",
      "|146852| aaabdelkader| 59|      1961|       33|         26|\n",
      "|500852|    aaadhemar| 67|      1953|       37|         29|\n",
      "|377554|   aaaemilian| 60|      1960|       27|         32|\n",
      "| 65634|      aaalagu| 61|      1959|       32|         29|\n",
      "|156662| aaaleksander| 61|      1959|       29|         31|\n",
      "|895784|    aaalexius| 58|      1962|       32|         25|\n",
      "|369612|      aaalois| 59|      1960|       35|         25|\n",
      "| 72451|     aaaluzio| 61|      1959|       26|         35|\n",
      "| 68001|    aaamabile| 55|      1964|       28|         27|\n",
      "| 70701|    aaanestis| 66|      1954|       36|         30|\n",
      "|507832|     aaanoosh| 62|      1957|       29|         33|\n",
      "|488522|      aaanwar| 64|      1956|       32|         32|\n",
      "|833602|    aaarlette| 67|      1952|       37|         30|\n",
      "| 28192|   aaarumugam| 67|      1952|       33|         34|\n",
      "|833032|aaarunachalam| 59|      1961|       29|         30|\n",
      "|856192|     aaarvind| 66|      1953|       33|         34|\n",
      "| 41488|     aaashish| 68|      1952|       39|         28|\n",
      "|490701|     aabaoqiu| 62|      1957|       34|         28|\n",
      "| 16711|     aabartek| 55|      1964|       26|         29|\n",
      "| 65114|     aabasant| 56|      1963|       23|         33|\n",
      "+------+-------------+---+----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee = id.select(\"id\", \"user\", \"age\", \"birth_year\", \"start_age\", \"year_worked\")\n",
    "employee = employee.sort(\"user\")\n",
    "employee.show(20)\n",
    "type(employee)\n",
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 1: The output file must be written in each file type shown below. The directory structure is defined below:\n",
    "BASE_DIR = /opt/spark_processing/data/employee\n",
    "FILE_TYPE = { csv | parquet | orc | avro | json }\n",
    "DATA = { employee }\n",
    "CURRENT_DATE = now()\n",
    "FILE_NAME = spark_$DATA_$CURRENT_DATE\n",
    "LOCATION = $BASE_DIR/$FILE_NAME/$DATA/$FILE_NAME\n",
    "\n",
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import coalesce\n",
    "curr_date = datetime.now().strftime('%Y_%m_%d')\n",
    "data = \"employee\"\n",
    "file_name_csv = \"spark_\"+data+\"_\"+curr_date+\".csv\"\n",
    "file_name_parquet = \"spark_\"+data+\"_\"+curr_date+\".parquet\"\n",
    "file_name_avro = \"spark_\"+data+\"_\"+curr_date+\".avro\"\n",
    "file_name_orc = \"spark_\"+data+\"_\"+curr_date+\".orc\"\n",
    "file_name_json = \"spark_\"+data+\"_\"+curr_date+\".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/'+file_name_csv\n",
    "employee.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"path\", path_csv).option(\"header\", \"true\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parquet = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/'+file_name_parquet\n",
    "employee.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").option(\"path\", path_parquet)\\\n",
    ".option(\"header\", \"true\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_orc = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/'+file_name_orc\n",
    "employee.coalesce(1).write.format(\"orc\").mode(\"overwrite\").option(\"path\", path_orc).option(\"header\", \"true\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_avro = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/'+file_name_avro\n",
    "employee.coalesce(1).write.format(\"avro\").mode(\"overwrite\").option(\"path\", path_avro).option(\"header\", \"true\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/'+file_name_json\n",
    "employee.coalesce(1).write.format(\"json\").mode(\"overwrite\").option(\"path\", path_json).option(\"header\", \"true\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b75d97c37405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtable_name1\u001b[0m  \u001b[1;31m#--if you supply table = table_name inside jdbc method, it throws error--\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0memployee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#.mode('append') throws error in console but still writes dataframe into MySql table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'mode'"
     ]
    }
   ],
   "source": [
    "\n",
    "table_name1 = \"spark_employees_\"+curr_date    \n",
    "database= \"analytics_tensor\"\n",
    "table_name = database+\".\"+table_name1  #--if you supply table = table_name inside jdbc method, it throws error--\n",
    "\n",
    "employee.write.jdbc(url = db_url, table = table_name, properties = db_properties)\\\n",
    ".mode('error').save()\n",
    "#.mode('append') throws error in console but still writes dataframe into MySql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
