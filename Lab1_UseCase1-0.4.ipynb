{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usecase-1**  \n",
    "\n",
    "Reading: Load employees table from MySQL database into Spark. \n",
    "\n",
    "Requirement: The report must include following information:   \n",
    "1. Calculate the employees current age.\n",
    "2. Calculate total number of years worked by employee.\n",
    "3. Find the age of employee when they are hired at the company.\n",
    "4. Show employee birth year.\n",
    "5. Create employee abbreviated name that contains 2 first character from last name and all character from first in lower case.\n",
    "6. Reverse employee number.\n",
    "\n",
    "Ordering: Sort the data by employee abbreviated name in ascending order.\n",
    "\n",
    "The ordinality, attributes name and type is defined below:\n",
    "6. id (type: integer)\n",
    "5. user (type: string)\n",
    "1. age (type: integer)\n",
    "4. birth_year (type: integer)\n",
    "3. start_age (type: integer)\n",
    "2. year_worked (type: integer)\n",
    "\n",
    "Output 1: The output file must be written in each file type shown below. The directory structure is defined below:   \n",
    "BASE_DIR = `/opt/spark_processing/data/employee`   \n",
    "FILE_TYPE = { `csv | parquet | orc | avro | json` }   \n",
    "DATA = { `employee` }   \n",
    "CURRENT_DATE = `now()`   \n",
    "FILE_NAME = `spark_$DATA_$CURRENT_DATE`   \n",
    "LOCATION = `$BASE_DIR/$FILE_NAME/$DATA/$FILE_NAME`     \n",
    " \n",
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reading: Load employees table from MySQL database into Spark.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\dipti\\\\spark-3.0.0-preview2-bin-hadoop3.2\\\\spark-3.0.0-preview2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('UseCase_1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config_filename = '.\\db_properties.ini'\n",
    "db_properties = {}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_filename)\n",
    "db_prop = config['mysql']\n",
    "db_url = db_prop ['url']#remove employees from 'url = jdbc:mysql://localhost:3306/employees' in db_properties.ini file\n",
    "db_properties ['driver'] = db_prop['driver']\n",
    "db_properties['user'] = db_prop['user']\n",
    "db_properties['password'] = db_prop['password']\n",
    "db_properties['timezone'] = db_prop ['timezone']\n",
    "#db_properties['database'] = db_prop['database'] # remove database = employees from db_properties.inifile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read.jdbc(url = db_url, table = 'employees',\\\n",
    "                               properties = db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|\n",
      "+------+----------+----------+---------+------+----------+---+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|\n",
      "+------+----------+----------+---------+------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the employees current age.\n",
    "from pyspark.sql.functions import floor\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import current_date, datediff\n",
    "curr_age = (datediff(current_date(), col(\"birth_date\")))/365\n",
    "age = employees_df.select('*', floor(curr_age).alias(\"age\").cast(\"Integer\"))\n",
    "age.show(5)\n",
    "age.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate total number of years worked by employee.\n",
    "from pyspark.sql.functions import round\n",
    "years = datediff(current_date(), \"hire_date\")/365\n",
    "year_worked = age.select('*', round(years, 0).cast('Integer').alias(\"year_worked\"))\n",
    "year_worked.show(5)\n",
    "year_worked.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the age of employee when they are hired at the company.\n",
    "age_when_hired = datediff(\"hire_date\", \"birth_date\")/365\n",
    "start_age = year_worked.select('*', floor(age_when_hired).cast('Integer').alias(\"start_age\"))\n",
    "start_age.show(5)\n",
    "start_age.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|birth_year|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|      1953|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|      1964|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|      1959|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|      1954|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|      1955|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show employee birth year.\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "birth_year = start_age.select('*',(regexp_replace(\"birth_date\", r'-\\d{2}-\\d{2}$', '')\\\n",
    "                                  .cast('Integer').alias(\"birth_year\")))\n",
    "birth_year.show(5)\n",
    "birth_year.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|birth_year|       user|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|      1953|   fageorgi|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|      1964|  sibezalel|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|      1959|    baparto|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|      1954|kochirstian|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|      1955|  makyoichi|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create employee abbreviated name that contains 2 first character from last name \n",
    "#and all character from first in lower case.\n",
    "from pyspark.sql.functions import concat, substring, lower, col\n",
    "\n",
    "l_name_sub = substring(\"last_name\", 0, 2)\n",
    "conct = lower(concat(l_name_sub, col(\"first_name\")))\n",
    "user = birth_year.select('*', conct.alias(\"user\"))\n",
    "user.show(5)\n",
    "user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+-----+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|age|year_worked|start_age|birth_year|       user|   id|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+-----+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66|         34|       32|      1953|   fageorgi|10001|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21| 56|         35|       21|      1964|  sibezalel|20001|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28| 60|         34|       26|      1959|    baparto|30001|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01| 66|         34|       32|      1954|kochirstian|40001|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12| 65|         31|       34|      1955|  makyoichi|50001|\n",
      "+------+----------+----------+---------+------+----------+---+-----------+---------+----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- emp_no: integer (nullable = true)\n",
      " |-- birth_date: date (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reverse employee number\n",
    "from pyspark.sql.functions import reverse\n",
    "\n",
    "id = user.select('*', reverse(\"emp_no\").cast('Integer').alias(\"id\"))\n",
    "id.show(5)\n",
    "id.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---+----------+---------+-----------+\n",
      "|    id|         user|age|birth_year|start_age|year_worked|\n",
      "+------+-------------+---+----------+---------+-----------+\n",
      "|146852| aaabdelkader| 59|      1961|       33|         26|\n",
      "|500852|    aaadhemar| 67|      1953|       37|         29|\n",
      "|377554|   aaaemilian| 60|      1960|       27|         32|\n",
      "| 65634|      aaalagu| 61|      1959|       32|         29|\n",
      "|156662| aaaleksander| 61|      1959|       29|         31|\n",
      "|895784|    aaalexius| 58|      1962|       32|         25|\n",
      "|369612|      aaalois| 59|      1960|       35|         25|\n",
      "| 72451|     aaaluzio| 61|      1959|       26|         35|\n",
      "| 68001|    aaamabile| 55|      1964|       28|         27|\n",
      "| 70701|    aaanestis| 66|      1954|       36|         30|\n",
      "|507832|     aaanoosh| 62|      1957|       29|         33|\n",
      "|488522|      aaanwar| 64|      1956|       32|         32|\n",
      "|833602|    aaarlette| 67|      1952|       37|         30|\n",
      "| 28192|   aaarumugam| 67|      1952|       33|         34|\n",
      "|833032|aaarunachalam| 59|      1961|       29|         30|\n",
      "|856192|     aaarvind| 66|      1953|       33|         34|\n",
      "| 41488|     aaashish| 68|      1952|       39|         28|\n",
      "|490701|     aabaoqiu| 62|      1957|       34|         28|\n",
      "| 16711|     aabartek| 55|      1964|       26|         29|\n",
      "| 65114|     aabasant| 56|      1963|       23|         33|\n",
      "+------+-------------+---+----------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- start_age: integer (nullable = true)\n",
      " |-- year_worked: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee = id.select(\"id\", \"user\", \"age\", \"birth_year\", \"start_age\", \"year_worked\")\n",
    "employee = employee.sort(\"user\")\n",
    "employee.show(20)\n",
    "type(employee)\n",
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 1: The output file must be written in each file type shown below. The directory structure is defined below:\n",
    "BASE_DIR = /opt/spark_processing/data/employee\n",
    "FILE_TYPE = { csv | parquet | orc | avro | json }\n",
    "DATA = { employee }\n",
    "CURRENT_DATE = now()\n",
    "FILE_NAME = spark_$DATA_$CURRENT_DATE\n",
    "LOCATION = $BASE_DIR/$FILE_NAME/$DATA/$FILE_NAME\n",
    "\n",
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "curr_date = datetime.now().strftime('%Y-%m-%d')\n",
    "data = \"employee\"\n",
    "file_name = \"spark_\"+data+\"_\"+curr_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/csv/'+file_name\n",
    "employee.write.format(\"csv\").mode(\"overwrite\").option(\"path\", path_csv).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parquet = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/parquet/'+file_name\n",
    "employee.write.mode(\"overwrite\").option(\"path\", path_parquet).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_orc = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/orc/'+file_name\n",
    "employee.write.format(\"orc\").mode(\"overwrite\").option(\"path\", path_orc).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_avro = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/avro/'+file_name\n",
    "employee.write.format(\"avro\").mode(\"overwrite\").option(\"path\", path_avro).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'C:/Users/dipti/PysparkLessons/spark_file_from_lab1usecase1/spark_employee/json/'+file_name\n",
    "employee.write.format(\"json\").mode(\"overwrite\").option(\"path\", path_json).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 2: Mysql table ( database name: analytics_tensor, table name: spark_employees_current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view 'analytics_tensor.spark_employees' already exists. SaveMode: ErrorIfExists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\spark-3.0.0-preview2-bin-hadoop3.2\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.0-preview2-bin-hadoop3.2\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o459.jdbc.\n: org.apache.spark.sql.AnalysisException: Table or view 'analytics_tensor.spark_employees' already exists. SaveMode: ErrorIfExists.;\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:173)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:211)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:208)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:828)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:828)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:309)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:664)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-cf33206c80f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#table_name = database.table_name1  --if you supply table = table_name inside jdbc method, it throws error--\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0memployee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"analytics_tensor.spark_employees\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#.mode('append') throws error in console but still writes dataframe into MySql table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.0-preview2-bin-hadoop3.2\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.0-preview2-bin-hadoop3.2\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.0-preview2-bin-hadoop3.2\\spark-3.0.0-preview2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnknownException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Table or view 'analytics_tensor.spark_employees' already exists. SaveMode: ErrorIfExists.;"
     ]
    }
   ],
   "source": [
    "\n",
    "#table_name1 = \"spark_employees_\"+curr_date    \n",
    "#database= \"analytics_tensor\"\n",
    "#table_name = database.table_name1  --if you supply table = table_name inside jdbc method, it throws error--\n",
    "\n",
    "employee.write.jdbc(url = db_url, table = \"analytics_tensor.spark_employees\", properties = db_properties)\\\n",
    ".mode('append').save()\n",
    "#.mode('append') throws error in console but still writes dataframe into MySql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
